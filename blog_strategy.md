# 네이버 블로그 크롤링 프로그램 기획서

## 1. 프로그램 개요
- 단일 실행 파일(.exe)로 제공되는 네이버 블로그 크롤링 프로그램
- 검색어와 페이지 수만 입력하면 자동으로 데이터 수집
- 구글 스프레드시트 API 선택적 사용 가능

## 2. 시스템 구성
```
[단일 실행 파일]
├── GUI (PyQt6)
├── 크롤링 엔진
└── 저장 관리자
    ├── 엑셀 저장 모듈
    └── 구글 스프레드시트 모듈 (선택)
```

## 3. 주요 기능

### 3.1 GUI 인터페이스
- 검색어 입력 필드
- 페이지 수 설정 (1-100)
- 저장 방식 선택
  - 로컬 엑셀 파일
  - 구글 스프레드시트 (선택)
- 진행 상황 표시 바
- 결과 미리보기 테이블

### 3.2 크롤링 기능
- 네이버 블로그 검색 결과 수집
- 수집 항목:
  - 블로그 제목
  - 포스트 내용 요약
  - 작성자
  - 작성일
  - URL
  - 수집일시

### 3.3 저장 기능
1. 기본 저장 (엑셀)
   - 자동으로 파일명 생성 (검색어_날짜.xlsx)
   - 지정 폴더에 저장

2. 구글 스프레드시트 저장 (선택)
   - 최초 실행 시 인증 정보 입력
   - 자동으로 새 시트 생성
   - 실시간 동기화

## 4. 기술 스택
- Python 3.9+
- PyQt6 (GUI)
- pandas (데이터 처리)
- openpyxl (엑셀 저장)
- google-api-python-client (선택)
- PyInstaller (실행 파일 생성)

## 5. 프로그램 흐름
```
1. 프로그램 실행
   ↓
2. 검색어 & 페이지 수 입력
   ↓
3. 저장 방식 선택
   ↓
4. 크롤링 시작
   ├── 진행 상황 표시
   └── 미리보기 테이블 업데이트
   ↓
5. 데이터 저장
   ├── 엑셀 파일 저장
   └── (선택) 구글 스프레드시트 동기화
   ↓
6. 완료 알림
```

## 6. 파일 구조
```
project/
├── src/
│   ├── main.py
│   ├── gui/
│   │   ├── main_window.py
│   │   └── components/
│   ├── crawler/
│   │   ├── naver_crawler.py
│   │   └── parser.py
│   ├── storage/
│   │   ├── excel_manager.py
│   │   └── sheets_manager.py
│   └── utils/
│       ├── config.py
│       └── helpers.py
├── resources/
│   ├── icons/
│   └── styles/
└── build/
    └── dist/
```

## 7. 사용자 경험
1. 첫 실행
   - 간단한 설정 가이드 제공
   - 구글 스프레드시트 사용 여부 선택
   - 저장 경로 설정

2. 일반 사용
   - 검색어 입력
   - 페이지 수 설정
   - 시작 버튼 클릭
   - 진행 상황 확인
   - 완료 후 결과 파일 자동 열기 옵션

## 8. 예외 처리
- 네트워크 오류 시 자동 재시도
- 잘못된 입력값 검증
- 파일 저장 오류 처리
- 구글 API 인증 오류 처리

## 9. 성능 고려사항
- 멀티스레딩으로 UI 응답성 유지
- 메모리 사용량 최적화
- 대용량 데이터 처리 방안
- 네이버 서버 부하 고려

이 구조라면:
1. 사용자는 exe 파일만 실행하면 됨
2. 추가 설치 없이 바로 사용 가능
3. 선택적으로 구글 스프레드시트 기능 사용 가능
4. 간단하고 직관적인 사용법

어떻게 생각하시나요? 수정하거나 추가하고 싶은 부분이 있으신가요?